{
  "metadata": {
    "name": "Note converted from Jupyter_2MAQV8Q6N",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#Bus Delay Analysis for Milestone 2\nUsing AWS EMR, Spark, S3 \u0026 Zeppelin\n\nThis notebook loads 46K rows of real-time bus arrival data collected\nfrom LTA DataMall API, performs big-data transformations, computes\ndelay patterns, ETA drift, and builds predictive models.\n\n#dataset\nStored on S3 as:\n`s3://transportbuddy-bucket/data/bus_arrivals.csv`\n\n#Steps\n1️ Load data from S3  \n2️ Feature engineering (hour/minute extraction)  \n3 ️ETA drift calculation (delay behaviour over time)  \n4a Average ETA by Hour of Day(overall buses)\n4b Average ETA Per Service Per Hour\n5 Average ETA Per-service hourly pattern\n6 ️Average ETA Drift by Service\n7 Binary Classification Model predicting the likelihood of a bus service being significantly delayed\n8Export all relevant files to create charts on frontend\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#STEP 1 - Load CSV from S3\n\nfrom pyspark.sql.functions import col, to_timestamp\n\n# Load dataset (5 days, 670k+ rows)\ndf_raw \u003d spark.read.csv(\n    \"s3://transportbuddy-bucket/data/bus_arrivals.csv\",\n    header\u003dTrue,\n    inferSchema\u003dTrue\n)\n\ndf \u003d df_raw \\\n    .withColumn(\"timestamp\", to_timestamp(\"collected_at\")) \\\n    .withColumn(\"bus_stop\", col(\"bus_stop_code\")) \\\n    .withColumn(\"service\", col(\"service_no\")) \\\n    .withColumn(\"eta_minutes\", col(\"minutes_away\")) \\\n    .withColumn(\"eta_seconds\", col(\"minutes_away\") * 60) \\\n    .select(\n        \"timestamp\",\n        \"bus_stop\",\n        \"service\",\n        \"eta_minutes\",\n        \"eta_seconds\",\n        \"operator\",\n        \"load\",\n        \"bus_type\",\n        \"latitude\",\n        \"longitude\"\n    )\n\ndf.show(5)\ndf.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#STEP 2 - Feature Engineering (Hour, Minute, Day)\n\nfrom pyspark.sql.functions import hour, minute, dayofweek\n\ndf2 \u003d df \\\n    .withColumn(\"hour\", hour(\"timestamp\")) \\\n    .withColumn(\"minute\", minute(\"timestamp\")) \\\n    .withColumn(\"weekday\", dayofweek(\"timestamp\"))\n\ndf2.show(5)\ndf2.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d STEP 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# NOTES\n# ETA Drift Calculation (in minutes)\n# This shows how the ETA prediction changes over time.\n\n# INTERPRETATION:\n# 1)eta_change_minutes \u003e 0 (ETA increased where bus is expected to arrive later than before)\n#(bus is more delayed than previous reading)\n#\n# 2)eta_change_minutes \u003c 0 ( ETA decreased where bus is expected to arrive earlier than before)\n#(prediction is moving forward → bus catching up)\n#\n# 3)eta_change_minutes \u003d 0  (ETA remained the same → prediction stable)\n\n# 4)eta_change_minutes \u003d NULL ( Occurs on the first record of each (bus_stop, service) group\n# because there is no previous ETA to compare.)\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, abs\n\nwindowSpec \u003d Window.partitionBy(\"bus_stop\", \"service\").orderBy(\"timestamp\")\n\n# Previous ETA in minutes\ndf3 \u003d df2.withColumn(\n    \"prev_eta_minutes\",\n    lag(\"eta_minutes\").over(windowSpec)\n)\n\n# ETA drift \u003d current ETA - previous ETA \ndf3 \u003d df3.withColumn(\n    \"eta_change_minutes\",\n    col(\"eta_minutes\") - col(\"prev_eta_minutes\")\n)\n\n# Added for richer insights: drift volatility (speed of change)\ndf3 \u003d df3.withColumn(\n    \"eta_volatility\",\n    abs(col(\"eta_change_minutes\"))\n)\n\nz.show(\n    df3.select(\n        \"timestamp\",\n        \"bus_stop\",\n        \"service\",\n        \"eta_minutes\",\n        \"prev_eta_minutes\",\n        \"eta_change_minutes\",\n        \"eta_volatility\"\n    ).limit(20)\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d STEP 4a \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# Average ETA by Hour of Day(overall buses)\n#\n# Expectations:\n# 7-9 AM: Higher average ETA (morning peak)\n# 5-7 PM: Higher average ETA (evening peak)\n# Off-peak hours: Lower and more stable ETA\n# Timeframe dataset was collected 5pm - 9 pm\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nfrom pyspark.sql.functions import avg, stddev\n\n# Now show weekday + hour to see patterns across 5 full days\navg_by_hour \u003d df2.groupBy(\"weekday\", \"hour\") \\\n    .agg(\n        avg(\"eta_minutes\").alias(\"avg_eta\"),\n        stddev(\"eta_minutes\").alias(\"eta_variability\")\n    ) \\\n    .orderBy(\"weekday\", \"hour\")\n\nz.show(avg_by_hour)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d STEP 4B \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# Per-Service Hourly ETA \n\nfrom pyspark.sql.functions import avg, stddev\n\navg_by_service_hour \u003d df2.groupBy(\"service\", \"weekday\", \"hour\") \\\n    .agg(\n        avg(\"eta_minutes\").alias(\"avg_eta\"),\n        stddev(\"eta_minutes\").alias(\"eta_variability\")\n    ) \\\n    .orderBy(\"service\", \"weekday\", \"hour\")\n\nz.show(avg_by_service_hour.limit(50))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d STEP 5 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# Median ETA + Service Stability Stats\n# Median ETA by Bus Service\n# Median gives the true typical ETA as average is distorted by outliers.\n#\n# To Identify which bus services have longer or shorter\n# predicted arrival times on average.\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\nfrom pyspark.sql.functions import percentile_approx, avg, stddev\n\nmedian_by_service \u003d df3.groupBy(\"service\") \\\n    .agg(\n        percentile_approx(\"eta_minutes\", 0.5).alias(\"median_eta\"),\n        avg(\"eta_minutes\").alias(\"avg_eta\"),\n        stddev(\"eta_minutes\").alias(\"eta_variability\"),\n        avg(\"eta_volatility\").alias(\"drift_volatility\")\n    ) \\\n    .orderBy(\"median_eta\", ascending\u003dFalse)\n\nz.show(median_by_service)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# -----------------------------------------------\n# STEP 6 - ETA Drift Analysis (Per Bus Service)\n#\n# Identify which bus services have more unstable ETA behaviour.\n# Positive drift - ETA keeps increasing (bus getting delayed)\n# Negative drift - ETA decreasing (bus catching up)\n# Near zero - stable predictions\n#\n# Analyse the average drift per service using eta_change_minutes\n# created in Step 3.\n# -----------------------------------------------\n\nfrom pyspark.sql.functions import avg, stddev\n\n# Use df3 because it contains:\n# - prev_eta_minutes\n# - eta_change_minutes\n# - eta_volatility (added in Step 3)\ndrift_by_service \u003d (\n    df3.filter(df3.eta_change_minutes.isNotNull())   # remove first rows (NULL)\n       .groupBy(\"service\")\n       .agg(\n           avg(\"eta_change_minutes\").alias(\"avg_eta_drift\"),\n           stddev(\"eta_change_minutes\").alias(\"drift_variability\"),\n           avg(\"eta_volatility\").alias(\"avg_volatility\")\n       )\n       .orderBy(\"avg_eta_drift\", ascending\u003dFalse)\n)\n\nz.show(drift_by_service)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\n# Step 7- FINAL ML MODEL\r\n\r\nfrom pyspark.sql.functions import col, percentile_approx, when, isnan\r\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\r\nfrom pyspark.ml.classification import RandomForestClassifier\r\nfrom pyspark.ml import Pipeline\r\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n\r\nprint(\"Starting FINAL ML...\")#To check\r\n\r\n#Feature Engineering \u0026 Label Creation (Data Preparation)\r\n\r\n# 1) Median ETA per service (used only for label creation)\r\nmedian_df \u003d df3.groupBy(\"service\").agg(\r\n    percentile_approx(\"eta_minutes\", 0.5).alias(\"median_eta\")\r\n)\r\ndf_ml \u003d df3.join(median_df, on\u003d\"service\", how\u003d\"left\")\r\n\r\n# 2) Create label\r\n# Label is 1 if current ETA is higher than the historical median ETA for that service\r\ndf_ml \u003d df_ml.withColumn(\r\n    \"label\",\r\n    (col(\"eta_minutes\") \u003e col(\"median_eta\")).cast(\"int\")\r\n)\r\n\r\n# 3) AGGRESSIVE CLEANING - Fix for NaN, null, and inf\r\nall_cols \u003d [\r\n    \"eta_minutes\", \"prev_eta_minutes\", \"eta_change_minutes\",\r\n    \"eta_volatility\", \"hour\", \"minute\", \"weekday\", \"bus_stop\"\r\n]\r\n\r\nfor c in all_cols:\r\n    df_ml \u003d df_ml.withColumn(\r\n        c,\r\n        when(col(c).isNull() | isnan(col(c)), 0.0)\r\n        .otherwise(col(c))\r\n        .cast(\"double\")\r\n    )\r\n\r\n# Filter out null labels\r\ndf_ml \u003d df_ml.filter(col(\"label\").isNotNull())\r\ndf_ml \u003d df_ml.filter(col(\"median_eta\").isNotNull())\r\n\r\nprint(f\"Records after cleaning: {df_ml.count()}\")\r\n\r\n\r\n# 4) Encode service as CONTINUOUS feature\r\nservice_indexer \u003d StringIndexer(\r\n    inputCol\u003d\"service\",\r\n    outputCol\u003d\"service_index\",\r\n    handleInvalid\u003d\"keep\"\r\n)\r\n\r\ndf_indexed \u003d service_indexer.fit(df_ml).transform(df_ml)\r\ndf_indexed \u003d df_indexed.withColumn(\"service_index\", col(\"service_index\").cast(\"double\"))\r\n\r\n\r\n#Model Setup\r\n\r\n# 5)Assemble features\r\nfeature_cols \u003d [\r\n    \"prev_eta_minutes\", \r\n    \"hour\", \r\n    \"minute\", \r\n    \"weekday\", \r\n    \"bus_stop\", \r\n    \"service_index\"\r\n]\r\n\r\nassembler \u003d VectorAssembler(\r\n    inputCols\u003dfeature_cols,\r\n    outputCol\u003d\"features\",\r\n    handleInvalid\u003d\"skip\"\r\n)\r\n\r\n\r\n# 6)Random Forest Classifier\r\nrf\u003d RandomForestClassifier(\r\n    featuresCol\u003d\"features\",\r\n    labelCol\u003d\"label\",\r\n    numTrees\u003d50,\r\n    maxDepth\u003d8,\r\n    seed\u003d42\r\n)\r\n\r\n# Pipeline\r\npipeline \u003d Pipeline(stages\u003d[assembler, rf])\r\n\r\n\r\n# 7) Split Data \r\ntrain, test \u003d df_indexed.randomSplit([0.70, 0.30], seed\u003d42)\r\nprint(f\"Train size: {train.count()}\")\r\nprint(f\"Test size: {test.count()}\")\r\n\r\n\r\n# 8)Fit model (using only the training data)\r\nprint(\"Fitting model...\")\r\nmodel \u003d pipeline.fit(train)\r\nprint(\"Model fitted successfully!\")\r\n\r\n\r\n# 9)Predict\r\npred \u003d model.transform(test)\r\n\r\n# 10)Evaluate\r\nevaluator \u003d BinaryClassificationEvaluator(\r\n    labelCol\u003d\"label\",\r\n    rawPredictionCol\u003d\"rawPrediction\",\r\n    metricName\u003d\"areaUnderROC\"\r\n)\r\nauc \u003d evaluator.evaluate(pred)\r\nprint(f\"\\n AUC \u003d {auc:.4f}\")\r\n\r\n\r\n# 11)Show predictions\r\npred.select(\r\n    \"service\", \"bus_stop\", \"eta_minutes\", \"prev_eta_minutes\",\r\n    \"eta_change_minutes\", \"eta_volatility\",\r\n    \"label\", \"prediction\", \"probability\"\r\n).show(5, truncate\u003dFalse)\r\n\r\nprint(\"\\nFINAL ML\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n# Export all datasets needed for frontend (SINGLE FILE EACH)\n# \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.sql.functions import col\n\nprint(\"Starting exports...\") #To check\n\n# 1)Export avg_by_hour\navg_by_hour.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/avg_by_hour/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n# 2)Export avg_by_service_hour\navg_by_service_hour.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/avg_by_service_hour/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n# 3)Export median_by_service\nmedian_by_service.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/median_by_service/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n# 4)Export drift_by_service\ndrift_by_service.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/drift_by_service/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n# 5)Export top 10 worst services\ntop10_worst \u003d drift_by_service.orderBy(\"avg_eta_drift\", ascending\u003dFalse).limit(10)\ntop10_worst.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/top10_worst/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n# 6)Export FULL ML predictions (vector -\u003e array)\npred_fixed \u003d pred.withColumn(\n    \"prob_array\",\n    vector_to_array(\"probability\")\n)\n\npred_export \u003d pred_fixed.select(\n    \"timestamp\",\n    \"service\",\n    \"bus_stop\",\n    \"eta_minutes\",\n    \"prev_eta_minutes\",\n    \"eta_change_minutes\",\n    \"eta_volatility\",\n    \"label\",\n    \"prediction\",\n    col(\"prob_array\")[1].alias(\"prob_delay_increase\")  # probability ETA will grow\n)\n\npred_export.coalesce(1).write.csv(\n    \"s3://transportbuddy-bucket/exports/full_predictions/\",\n    mode\u003d\"overwrite\",\n    header\u003dTrue\n)\n\n\nprint(\"All 6 export files successfully written to S3 (single CSV each).\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n%spark.pyspark\n"
    }
  ]
}